{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUBYLFIedb_3"
   },
   "source": [
    "# Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using the Google Colab, mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTaH4eG3db_4"
   },
   "source": [
    "## 1. Load the data\n",
    "\n",
    "For training and testing the traffic sign classifier, the GTSRB dataset (German Traffic Sign Recognition Benchmark) provided by the Institut fÃ¼r Neuroinformatik group is used. Link to the full dataset is [here](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "However, here we have only used the pickled dataset with images resized to 32x32. This dataset needs to be downloaded and extracted in the current workspace directory. Download link to the pickled dataset is [here](https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic-signs-data.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0H9X1_mydb_5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_traffic_sign_data(training_file, testing_file):\n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    return train, test\n",
    "\n",
    "# Load pickled data\n",
    "train, test = load_traffic_sign_data('/content/drive/My Drive/Colab Notebooks/traffic-signs-data/train.p', '/content/drive/My Drive/Colab Notebooks/traffic-signs-data/test.p')\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "y_train = y_train.astype('int64')\n",
    "y_test = y_test.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrPVrchadb_7"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1561317192376,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "dPSK4k8Mdb_8",
    "outputId": "d2ef89bc-43b2-4595-dbd0-0df6d5ff3690"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of examples\n",
    "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many classes?\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples  =\", n_test)\n",
    "print(\"Image data shape  =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PClih5Z9db_-"
   },
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s).\n",
    "\n",
    "- First we can visualize some images sampled from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6436,
     "status": "ok",
     "timestamp": 1561317196716,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "fI9coNdHdcAA",
    "outputId": "9b798884-35ba-45ba-ae84-592380b3e5a1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show a random sample from each class of the traffic sign dataset\n",
    "rows, cols = 4, 12\n",
    "fig, ax_array = plt.subplots(rows, cols)\n",
    "plt.suptitle('RANDOM SAMPLES FROM TRAINING SET (one for each class)')\n",
    "for class_idx, ax in enumerate(ax_array.ravel()):\n",
    "    if class_idx < n_classes:\n",
    "        # show a random image of the current class\n",
    "        cur_X = X_train[y_train == class_idx]\n",
    "        cur_img = cur_X[np.random.randint(len(cur_X))]\n",
    "        ax.imshow(cur_img)\n",
    "        ax.set_title('{:02d}'.format(class_idx))\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "# hide both x and y ticks\n",
    "plt.setp([a.get_xticklabels() for a in ax_array.ravel()], visible=False)\n",
    "plt.setp([a.get_yticklabels() for a in ax_array.ravel()], visible=False)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G65SH3xLdcAD"
   },
   "source": [
    "- We can also get the idea of how these classes are distributed in both training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6344,
     "status": "ok",
     "timestamp": 1561317196731,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "vT-mZ7Z7dcAH",
    "outputId": "d2dd676a-53f9-485a-dd21-c070ee7f9edb"
   },
   "outputs": [],
   "source": [
    "# bar-chart of classes distribution\n",
    "train_distribution, test_distribution = np.zeros(n_classes), np.zeros(n_classes)\n",
    "for c in range(n_classes):\n",
    "    train_distribution[c] = np.sum(y_train == c) / n_train\n",
    "    test_distribution[c] = np.sum(y_test == c) / n_test\n",
    "fig, ax = plt.subplots()\n",
    "col_width = 0.5\n",
    "bar_train = ax.bar(np.arange(n_classes), train_distribution, width=col_width, color='r')\n",
    "bar_test = ax.bar(np.arange(n_classes)+col_width, test_distribution, width=col_width, color='b')\n",
    "ax.set_ylabel('PERCENTAGE OF PRESENCE')\n",
    "ax.set_xlabel('CLASS LABEL')\n",
    "ax.set_title('Classes distribution in traffic-sign dataset')\n",
    "ax.set_xticks(np.arange(0, n_classes, 5)+col_width)\n",
    "ax.set_xticklabels(['{:02d}'.format(c) for c in range(0, n_classes, 5)])\n",
    "ax.legend((bar_train[0], bar_test[0]), ('train set', 'test set'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahriJ-5vdcAI"
   },
   "source": [
    "----\n",
    "\n",
    "## Design a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5FDnjMpdcAJ"
   },
   "source": [
    "**Feature preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP93gT3HdcAK"
   },
   "source": [
    "Following this paper [[Sermanet, LeCun]](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf) I employed three main steps of feature preprocessing:\n",
    "\n",
    "1) *each image is converted from RGB to YUV color space, then only the Y channel is used.* This choice can sound at first suprising, but the cited paper shows how this choice leads to the best performing model. This is slightly counter-intuitive, but if we think about it arguably we are able to distinguish all the traffic signs just by looking to the grayscale image.\n",
    "\n",
    "2) *contrast of each image is adjusted by means of histogram equalization*. This is to mitigate the numerous situation in which the image contrast is really poor.\n",
    "a\n",
    "3) *each image is centered on zero mean and divided for its standard deviation*. This feature scaling is known to have beneficial effects on the gradient descent performed by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIaqTWWYdcAK"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "def preprocess_features(X, equalize_hist=True):\n",
    "\n",
    "    # convert from RGB to YUV\n",
    "    X = np.array([np.expand_dims(cv2.cvtColor(rgb_img, cv2.COLOR_RGB2YUV)[:, :, 0], 2) for rgb_img in X])\n",
    "\n",
    "    # adjust image contrast\n",
    "    if equalize_hist:\n",
    "        X = np.array([np.expand_dims(cv2.equalizeHist(np.uint8(img)), 2) for img in X])\n",
    "\n",
    "    X = np.float32(X)\n",
    "\n",
    "    # standardize features\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= (np.std(X, axis=0) + np.finfo('float32').eps)\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train_norm = preprocess_features(X_train)\n",
    "X_test_norm = preprocess_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XCW6gxhdcAN"
   },
   "source": [
    "### Dataset Augmentation\n",
    "\n",
    "Use pytorch \"transforms\" to augment the dataset by randomly doing affine transformations on the images. Also including the ToTensor() to convert the images to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAJMAjd2dcAN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# split into train and validation\n",
    "VAL_RATIO = 0.2\n",
    "X_train_norm, X_val_norm, y_train, y_val = train_test_split(X_train_norm, y_train, test_size=VAL_RATIO, random_state=0)\n",
    "\n",
    "image_transform = transforms.Compose([transforms.ToPILImage(),                                   \n",
    "                                     transforms.RandomAffine(degrees=15.,\n",
    "                                                            translate=(0.1,0.1),\n",
    "                                                            scale=(0.8,1.2),\n",
    "                                                            ),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZpzfD98oU6m"
   },
   "outputs": [],
   "source": [
    "standard_transform = transforms.Compose([transforms.ToPILImage(),                                   \n",
    "                                       transforms.ToTensor(),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9pCCv-pdcAP"
   },
   "source": [
    "### Visualize random tranformations on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10538,
     "status": "ok",
     "timestamp": 1561317201030,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "HYvffyjRdcAQ",
    "outputId": "17a33101-882a-476c-a48b-60f810f33b22"
   },
   "outputs": [],
   "source": [
    "# take a random image from the training set\n",
    "img_rgb = X_train[100]\n",
    "\n",
    "# plot the original image\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Example of RGB image (class = {})'.format(y_train[0]))\n",
    "plt.show()\n",
    "\n",
    "# plot some randomly augmented images\n",
    "rows, cols = 4, 10\n",
    "fig, ax_array = plt.subplots(rows, cols)\n",
    "for ax in ax_array.ravel():\n",
    "    augmented_img = image_transform(img_rgb)\n",
    "    ax.imshow(augmented_img.numpy().transpose((1,2,0)))\n",
    "plt.setp([a.get_xticklabels() for a in ax_array.ravel()], visible=False)\n",
    "plt.setp([a.get_yticklabels() for a in ax_array.ravel()], visible=False)\n",
    "plt.suptitle('Random examples of data augmentation (starting from the previous image)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4TsoRSKdcAT"
   },
   "source": [
    "### Create a pytorch dataset using torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWpNbZqPdcAT"
   },
   "outputs": [],
   "source": [
    "def create_tensor_dataset(X_array, y_array, transform):\n",
    "    X_torch = torch.stack([transform(img) for img in X_array])\n",
    "    y_torch = torch.from_numpy(y_array)\n",
    "    dataset = torch.utils.data.TensorDataset(X_torch, y_torch)\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSTwB1OCdcAW"
   },
   "outputs": [],
   "source": [
    "# Not using image_transform for training because PyTorch tranform is creating black backgrounds for rotation and shift transforms whereas TF's tranforms seem to adjust the images without black backgrounds\n",
    "# @ToDo Solve the black background issue of pytorch tranforms and test again.\n",
    "\n",
    "train_set = create_tensor_dataset(X_train_norm, y_train, standard_transform)\n",
    "val_set = create_tensor_dataset(X_val_norm, y_val, standard_transform)\n",
    "test_set = create_tensor_dataset(X_test_norm, y_test, standard_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocwvUYxUdcAZ"
   },
   "source": [
    "### Define a dataloader to create train, validation and test pytorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1Nnx-xPdcAZ"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=2)\n",
    "    return(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVX837JCdcAb"
   },
   "source": [
    "### Define the convolutional network \n",
    "As mentioned in the paper, the network will have two conv layers with relu activations each separated by maxpooling layers. Conv layers are padded to maintain the input size. Dropout layers need to be added.\n",
    "Finally two fully-connected layers are added. The outputs from both convolutional layers are flattened and concatenated before feeding to the FC layers (as explained in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOK7EdcqdcAc"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class my_net(torch.nn.Module):\n",
    "    def __init__(self, n_classes, dropout_prob=0.2):\n",
    "        super(my_net,self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(16*16*64 + 8*8*128,64)\n",
    "        self.fc2 = torch.nn.Linear(64,n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # computes the activation of the first convolution\n",
    "        # input images are (1,32,32). height and width are preserved. Output size = (64,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        # size changes from (64,32,32) to (64,16,16)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "                \n",
    "        # flatten and save output to feed the low level features of this 1st conv layer to the dense FC layers\n",
    "        # size of x_conv1 = (1,16*16*64)\n",
    "        x_conv1 = x.view(-1,16*16*64)\n",
    "        \n",
    "        # computes the activation of the second convolution\n",
    "        # size changes from (64,16,16) to (128,16,16) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # size changes from (128,16,16) to (128,8,8)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Flatten the input to feed the FC layers\n",
    "        # Size changes from (128,8,8,) to (1,8*8*128)\n",
    "        x = x.view(-1,8*8*128)\n",
    "                \n",
    "        # Concatenate with the flattened conv1 layer output\n",
    "        # size changes from (1,8*8*128) to (1, 16*16*64 + 8*8*128)\n",
    "        x = torch.cat((x_conv1, x), dim=1)\n",
    "        \n",
    "        # FC1. Size changes from (1,16*16*64 + 8*8*128) to (1,64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # FC2. Size changes from (1,64) to (1,n_classes])\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUG53LsUdcAe"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return (loss,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU is available or using Google Colab's GPU, make this parameter True to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyjlsmE607F-"
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the accuracy function\n",
    "use net.eval() to turn off dropout and put the model in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKyvBElndcAh"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def calculate_accuracy(net, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Put the model in eval mode so that dropout layers are ineffective\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            # If GPU available\n",
    "            if(use_cuda & torch.cuda.is_available()):\n",
    "              inputs = inputs.to('cuda')\n",
    "              labels = labels.to('cuda')\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return(100 * correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXWjXi_EdcAk"
   },
   "source": [
    "###  To Store the accuracies and losses during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8fsPL5NdcAm"
   },
   "outputs": [],
   "source": [
    "train_accuracies, val_accuracies = [], []\n",
    "train_losses, val_losses = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiWFxXFwdcAs"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    print(\"=========== HYPERPARAMETERS ===========\")\n",
    "    print(\"batch size = \", batch_size)\n",
    "    print(\"epochs = \",n_epochs)\n",
    "    print(\"learning rate = \",learning_rate)\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Get training and validation data\n",
    "    train_loader = get_data_loader(train_set, batch_size)\n",
    "    val_loader = get_data_loader(val_set, batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    # Create loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    # Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Put the model in train mode so that dropout layers are effective\n",
    "        net.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_losses = []\n",
    "        print_every = n_batches//10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            # Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Wrap them in a variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            # If GPU available\n",
    "            if(use_cuda & torch.cuda.is_available()):\n",
    "              inputs = inputs.to('cuda')\n",
    "              labels = labels.to('cuda')\n",
    "            \n",
    "            # Set the parameters gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass and loss\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            \n",
    "            # Backprop\n",
    "            loss_size.backward()\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get running loss\n",
    "            running_loss += loss_size.item()\n",
    "            total_train_loss += loss_size.item()\n",
    "            \n",
    "            if (i+1)%(print_every+1) == 0:\n",
    "                print(\"Epoch: {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                    epoch+1, int(100*(i+1)/n_batches), running_loss/print_every, time.time() - start_time))\n",
    "                running_losses.append(running_loss/print_every)\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "        \n",
    "        # Append running training losses to list\n",
    "        train_losses.append(running_losses)\n",
    "        \n",
    "        # After each epoch, calculate loss on the val set\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        # Put the model in eval mode so that dropout layers are ineffective\n",
    "        net.eval()\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            # Wrap variables in Variable objects\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            # If GPU available\n",
    "            if(use_cuda & torch.cuda.is_available()):\n",
    "              inputs = inputs.to('cuda')\n",
    "              labels = labels.to('cuda')\n",
    "            \n",
    "            # Forward Pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.item()\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss/len(val_loader)))\n",
    "        \n",
    "        # Append the val loss to list\n",
    "        val_losses.append(total_val_loss/(len(val_loader)))\n",
    "        \n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calculate_accuracy(net, train_loader)\n",
    "        val_accuracy = calculate_accuracy(net, val_loader)\n",
    "        print('Train Accuracy = {:.3f} - Validation Accuracy: {:.3f}'.format(train_accuracy, val_accuracy))\n",
    "        \n",
    "        # Append the accuracies to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Save the epoch\n",
    "        #torch.save(net.state_dict(), os.path.join(os.getcwd()+'/models/', 'epoch-{}.pth'.format(epoch)))\n",
    "    \n",
    "    print(\"Training finished. Took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13020,
     "status": "ok",
     "timestamp": 1561317203621,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "vnzlQnl0yfM3",
    "outputId": "beed4b92-5195-44ec-d5aa-60b68414a27f"
   },
   "outputs": [],
   "source": [
    "net = my_net(n_classes, dropout_prob=0.2)\n",
    "if(use_cuda & torch.cuda.is_available()):\n",
    "  print(\"GPU Available\")\n",
    "  net.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8O5MG32dcAu"
   },
   "source": [
    "### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 244937,
     "status": "ok",
     "timestamp": 1561317435638,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "S4dAhdaedcAu",
    "outputId": "528b8c56-9783-47a6-a5ae-ce3ba795398a"
   },
   "outputs": [],
   "source": [
    "trainNet(net, batch_size=32, n_epochs=30, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1561317465435,
     "user": {
      "displayName": "Vamshi Chowdary",
      "photoUrl": "https://lh6.googleusercontent.com/-jL-RnimPjas/AAAAAAAAAAI/AAAAAAAAATU/zG6gqeOI_yA/s64/photo.jpg",
      "userId": "09907547561072992244"
     },
     "user_tz": -330
    },
    "id": "Md7h_FkndcA_",
    "outputId": "8b755f73-e635-4343-d987-58b0288b19f8"
   },
   "outputs": [],
   "source": [
    "test_loader = get_data_loader(test_set, batch_size=128)\n",
    "test_accuracy = calculate_accuracy(net, test_loader)\n",
    "print(\"Accuracy on the test dataset = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9pOfk2rdcBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNaNDzTbdcBf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZJClcxb7RT6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRZgG9aq7Tpu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQKj_jXC7Zrk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZh6fCzx7baI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wz2IODOS7eKA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZmRZ8z37gDA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Traffic Sign Recognition Classifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
